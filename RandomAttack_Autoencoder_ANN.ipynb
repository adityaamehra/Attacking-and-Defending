{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb75955",
   "metadata": {
    "id": "efb75955"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors, kneighbors_graph\n",
    "from sklearn.manifold import trustworthiness\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.manifold import MDS\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import queue\n",
    "import json\n",
    "\n",
    "\n",
    "# Antenna definitions\n",
    "ASSIGNMENTS = [\n",
    "\t[0, 13, 31, 29, 3, 7, 1, 12 ],\n",
    "\t[30, 26, 21, 25, 24, 8, 22, 15],\n",
    "\t[28, 5, 10, 14, 6, 2, 16, 18],\n",
    "\t[19, 4, 23, 17, 20, 11, 9, 27]\n",
    "]\n",
    "\n",
    "ANTENNACOUNT = np.sum([len(antennaArray) for antennaArray in ASSIGNMENTS])\n",
    "\n",
    "# Optimized version: Calibration constants precomputed once\n",
    "def load_calibrate_timedomain(path, offset_path):\n",
    "    with open(offset_path, \"r\") as offsetfile:\n",
    "        offsets = json.load(offsetfile)\n",
    "\n",
    "    # Precompute the calibration tensors\n",
    "    fft_len = 1024\n",
    "    freq_range = 2 * np.pi * np.arange(fft_len, dtype=np.float32) / fft_len\n",
    "    sto_offset = tf.constant(offsets[\"sto\"], dtype=tf.float32)[:, None] * freq_range\n",
    "    cpo_offset = tf.constant(offsets[\"cpo\"], dtype=tf.float32)[:, None]\n",
    "    calib_multiplier = tf.exp(tf.complex(0.0, sto_offset + cpo_offset))\n",
    "\n",
    "    def record_parse_function(proto):\n",
    "        record = tf.io.parse_single_example(proto, {\n",
    "            \"csi\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"pos-tachy\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"time\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        })\n",
    "\n",
    "        csi = tf.io.parse_tensor(record[\"csi\"], out_type=tf.float32)\n",
    "        csi = tf.complex(csi[:, :, 0], csi[:, :, 1])\n",
    "        csi = tf.signal.fftshift(csi, axes=1)\n",
    "\n",
    "        position = tf.io.parse_tensor(record[\"pos-tachy\"], out_type=tf.float64)\n",
    "        time = record[\"time\"]\n",
    "        return csi, position[:2], time\n",
    "\n",
    "    def apply_calibration(csi, pos, time):\n",
    "        csi = tf.multiply(csi, calib_multiplier)\n",
    "        return csi, pos, time\n",
    "\n",
    "    def csi_time_domain(csi, pos, time):\n",
    "        csi = tf.signal.fftshift(tf.signal.ifft(tf.signal.fftshift(csi, axes=1)), axes=1)\n",
    "        return csi, pos, time\n",
    "\n",
    "    def cut_out_taps(tap_start, tap_stop):\n",
    "        def func(csi, pos, time):\n",
    "            return csi[:, tap_start:tap_stop], pos, time\n",
    "        return func\n",
    "\n",
    "    def order_by_antenna(csi, pos, time):\n",
    "        csi = tf.stack([tf.gather(csi, indices) for indices in ASSIGNMENTS])\n",
    "        return csi, pos, time\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(path)\n",
    "    dataset = dataset.map(record_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(apply_calibration, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(csi_time_domain, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(cut_out_taps(507, 520), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(order_by_antenna, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "inputpaths = [\n",
    "\t{\n",
    "\t\t\"tfrecords\" : \"dichasus/dichasus-cf02.tfrecords\",\n",
    "\t\t\"offsets\" : \"dichasus/reftx-offsets-dichasus-cf02.json\"\n",
    "\t},\n",
    "\t{\n",
    "\t\t\"tfrecords\" : \"dichasus/dichasus-cf03.tfrecords\",\n",
    "\t\t\"offsets\" : \"dichasus/reftx-offsets-dichasus-cf03.json\"\n",
    "\t},\n",
    "\t{\n",
    "\t\t\"tfrecords\" : \"dichasus/dichasus-cf04.tfrecords\",\n",
    "\t\t\"offsets\" : \"dichasus/reftx-offsets-dichasus-cf04.json\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "full_dataset = load_calibrate_timedomain(inputpaths[0][\"tfrecords\"], inputpaths[0][\"offsets\"])\n",
    "\n",
    "for path in inputpaths[1:]:\n",
    "\tfull_dataset = full_dataset.concatenate(load_calibrate_timedomain(path[\"tfrecords\"], path[\"offsets\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf28e7",
   "metadata": {
    "id": "3aaf28e7"
   },
   "outputs": [],
   "source": [
    "groundtruth_positions = []\n",
    "csi_time_domain = []\n",
    "timestamps = []\n",
    "for csi, pos, time in full_dataset.batch(1000):\n",
    "\tcsi_time_domain.append(csi.numpy())\n",
    "\tgroundtruth_positions.append(pos.numpy())\n",
    "\ttimestamps.append(time.numpy())\n",
    "\n",
    "csi_time_domain = np.concatenate(csi_time_domain)\n",
    "groundtruth_positions = np.concatenate(groundtruth_positions)\n",
    "timestamps = np.concatenate(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "csi_time_domain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f654351",
   "metadata": {
    "id": "3f654351"
   },
   "outputs": [],
   "source": [
    "def feature_extraction_F1(csi_matrix):\n",
    "    csi_ifft = np.fft.ifft(csi_matrix, axis=1)\n",
    "    R = csi_ifft * np.conj(csi_ifft)\n",
    "    r = R.flatten()\n",
    "    r_real = np.real(r)\n",
    "    r_imag = np.imag(r)\n",
    "    r_R = np.concatenate((r_real, r_imag))\n",
    "    feature_vector = r_R / np.linalg.norm(r_R)\n",
    "    return feature_vector\n",
    "m=csi_time_domain.shape[0]\n",
    "reshaped_csi = csi_time_domain.reshape(m, 32, 13)\n",
    "features_F1 = np.array([feature_extraction_F1(csi) for csi in reshaped_csi])\n",
    "print(\"Shape of Features (F1):\", features_F1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3ab63",
   "metadata": {
    "id": "8bc3ab63"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, x1, y_train, y1 = train_test_split(features_F1, groundtruth_positions, test_size=0.4, random_state=42)\n",
    "X_test,X_pred, y_test, y_pred = train_test_split(x1, y1, test_size=0.5, random_state=42)\n",
    "print(\"Training features shape:\", X_train.shape)\n",
    "print(\"Testing features shape:\", X_test.shape)\n",
    "print(\"Training positions shape:\", y_train.shape)\n",
    "print(\"Testing positions shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_x_train= y_train[:, 0].reshape(-1,1)\n",
    "pos_y_train= y_train[:, 1].reshape(-1,1)\n",
    "pos_x_test= y_test[:, 0].reshape(-1,1)\n",
    "pos_y_test= y_test[:, 1].reshape(-1,1)\n",
    "pos_x_pred= y_pred[:, 0].reshape(-1,1)\n",
    "pos_y_pred= y_pred[:, 1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "def build_grid(grid_size=30, space_bounds=None):\n",
    "    (xmin, xmax), (ymin, ymax) = space_bounds\n",
    "    x_lin = np.linspace(xmin, xmax, grid_size)\n",
    "    y_lin = np.linspace(ymin, ymax, grid_size)\n",
    "        \n",
    "    xv, yv = np.meshgrid(x_lin, y_lin)\n",
    "    grid_points = np.stack([xv.ravel(), yv.ravel()], axis=1)\n",
    "    return grid_points\n",
    "\n",
    "def compute_soft_probability_maps(positions, grid_points, temperature=1.0):\n",
    "    N = positions.shape[0]\n",
    "    K = grid_points.shape[0]\n",
    "    dists = np.linalg.norm(positions[:, np.newaxis, :] - grid_points[np.newaxis, :, :], axis=2)\n",
    "    sim_scores = -dists / temperature\n",
    "    prob_maps = softmax(sim_scores, axis=1)\n",
    "    return prob_maps\n",
    "\n",
    "def estimate_positions_from_maps(prob_maps, grid_points):\n",
    "    return np.matmul(prob_maps, grid_points)\n",
    "\n",
    "def generate_all_probability_maps(positions, grid_size=30, temperature=1.0):\n",
    "    x_min, x_max = positions[:, 0].min(), positions[:, 0].max()\n",
    "    y_min, y_max = positions[:, 1].min(), positions[:, 1].max()\n",
    "\n",
    "    grid_points = build_grid(grid_size, space_bounds=((x_min, x_max), (y_min, y_max)))\n",
    "    print(\"Grid points shape:\", grid_points.shape)\n",
    "    prob_maps = compute_soft_probability_maps(positions, grid_points, temperature=temperature)\n",
    "\n",
    "    est_locations = estimate_positions_from_maps(prob_maps, grid_points)\n",
    "\n",
    "    return prob_maps, est_locations, grid_points\n",
    "\n",
    "prob_maps, est_positions, grid_points = generate_all_probability_maps(y_train)\n",
    "\n",
    "print(prob_maps.shape)\n",
    "print(est_positions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a2082",
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_train-est_positions).std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cbb3a0",
   "metadata": {
    "id": "15cbb3a0"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "class NeuralNetwork(tf.keras.Model):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = layers.Dense(2048, activation='relu', input_dim=input_size)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.fc2 = layers.Dense(2048, activation='relu')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.fc3 = layers.Dense(2048, activation='relu')\n",
    "        self.fc4 = layers.Dense(2048, activation='relu')\n",
    "        self.fc5 = layers.Dense(2048, activation='relu')\n",
    "        # Output layer: output_size = number of grid points, softmax for probability\n",
    "        self.fc6 = layers.Dense(output_size, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "# Set output_size to the number of grid points (e.g., 900 for 30x30 grid)\n",
    "output_size = grid_points.shape[0]  # grid_points from your earlier code\n",
    "model = NeuralNetwork(input_size=832, output_size=output_size)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',  # or 'kullback_leibler_divergence'\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f55906",
   "metadata": {
    "id": "46f55906"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, prob_maps, epochs=30, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec2ed3",
   "metadata": {
    "id": "7eec2ed3"
   },
   "outputs": [],
   "source": [
    "predicted_maps = model.predict(X_test)\n",
    "predicted_positions = np.matmul(predicted_maps, grid_points)\n",
    "mae = np.mean(np.abs(y_test - predicted_positions))\n",
    "print(\"Mean Absolute Error (Position Predictions):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7d647",
   "metadata": {
    "id": "3cf7d647"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "def build_denoising_autoencoder(input_dim):\n",
    "    \"\"\"\n",
    "    Builds a denoising autoencoder model.\n",
    "\n",
    "    Args:\n",
    "        input_dim: Dimension of the input features (e.g., 832 for features_F1).\n",
    "\n",
    "    Returns:\n",
    "        autoencoder: The denoising autoencoder model.\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_data = Input(shape=(input_dim,), name=\"input\")\n",
    "\n",
    "    # Encoder\n",
    "    x = Dense(512, activation=\"relu\", name=\"encoder_dense_1\")(input_data)\n",
    "    x = BatchNormalization(name=\"encoder_bn_1\")(x)\n",
    "    x = Dropout(0.2, name=\"encoder_dropout_1\")(x)\n",
    "\n",
    "    x = Dense(256, activation=\"relu\", name=\"encoder_dense_2\")(x)\n",
    "    x = BatchNormalization(name=\"encoder_bn_2\")(x)\n",
    "    x = Dropout(0.2, name=\"encoder_dropout_2\")(x)\n",
    "\n",
    "    x = Dense(128, activation=\"relu\", name=\"encoder_dense_3\")(x)\n",
    "    x = BatchNormalization(name=\"encoder_bn_3\")(x)\n",
    "    x = Dropout(0.2, name=\"encoder_dropout_3\")(x)\n",
    "\n",
    "    x = Dense(64, activation=\"relu\", name=\"encoder_dense_4\")(x)\n",
    "    x = BatchNormalization(name=\"encoder_bn_4\")(x)\n",
    "    x = Dropout(0.2, name=\"encoder_dropout_4\")(x)\n",
    "\n",
    "    encoded = Dense(32, activation=\"relu\", name=\"encoder_latent\")(x)  # Latent space\n",
    "\n",
    "    # Decoder\n",
    "    x = Dense(64, activation=\"relu\", name=\"decoder_dense_1\")(encoded)\n",
    "    x = BatchNormalization(name=\"decoder_bn_1\")(x)\n",
    "    x = Dropout(0.2, name=\"decoder_dropout_1\")(x)\n",
    "\n",
    "    x = Dense(128, activation=\"relu\", name=\"decoder_dense_2\")(x)\n",
    "    x = BatchNormalization(name=\"decoder_bn_2\")(x)\n",
    "    x = Dropout(0.2, name=\"decoder_dropout_2\")(x)\n",
    "\n",
    "    x = Dense(256, activation=\"relu\", name=\"decoder_dense_3\")(x)\n",
    "    x = BatchNormalization(name=\"decoder_bn_3\")(x)\n",
    "    x = Dropout(0.2, name=\"decoder_dropout_3\")(x)\n",
    "\n",
    "    x = Dense(512, activation=\"relu\", name=\"decoder_dense_4\")(x)\n",
    "    x = BatchNormalization(name=\"decoder_bn_4\")(x)\n",
    "    x = Dropout(0.2, name=\"decoder_dropout_4\")(x)\n",
    "\n",
    "    decoded = Dense(input_dim, activation=\"linear\", name=\"decoder_output\")(x)  # Reconstruct clean features\n",
    "\n",
    "    # Autoencoder model\n",
    "    autoencoder = Model(input_data, decoded, name=\"DenoisingAutoencoder\")\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "# Build the denoising autoencoder\n",
    "input_dim = 832  # Dimension of the input features\n",
    "autoencoder = build_denoising_autoencoder(input_dim)\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Print the autoencoder summary\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238f444",
   "metadata": {
    "id": "1238f444"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from scipy.signal import convolve\n",
    "\n",
    "# mae_noise_list = []\n",
    "# mae_denoise_list = []\n",
    "# lp_values = []\n",
    "\n",
    "# groundtruth_maps, est_positions, grid_points = generate_all_probability_maps(groundtruth_positions)\n",
    "# for x in range(1, 10):\n",
    "#     lp_values.append(x)\n",
    "\n",
    "#     # Parameters\n",
    "#     W = 1024\n",
    "#     modulation_order = 16\n",
    "#     C = W // 8\n",
    "#     L = 13\n",
    "#     Lp = x\n",
    "#     noise_power = 0.1\n",
    "#     B = 32\n",
    "#     # Generate frequency-domain symbols\n",
    "#     data = np.random.randint(0, modulation_order, W)\n",
    "#     qam_symbols = (2 * (data % 4) - 3) + 1j * (2 * (data // 4) - 3)\n",
    "#     qam_symbols /= np.sqrt(10)\n",
    "\n",
    "#     time_domain_signal = np.fft.ifft(qam_symbols)\n",
    "\n",
    "#     # Zero-mean Gaussian for amplitude\n",
    "#     A_k = np.random.normal(loc=0.0, scale=1.0, size=Lp)\n",
    "#     # Zero-mean Gaussian for phase\n",
    "#     phi_k = np.random.normal(loc=0.0, scale=1.0, size=Lp)\n",
    "\n",
    "#     p_bar = A_k * np.exp(1j * phi_k)\n",
    "#     p_bar /= np.linalg.norm(p_bar)\n",
    "\n",
    "#     perturbed_signal = np.convolve(p_bar, time_domain_signal, mode=\"full\")[:W]\n",
    "#     cyclic_prefix = perturbed_signal[-C:]\n",
    "#     transmit_signal = np.concatenate([cyclic_prefix, perturbed_signal])\n",
    "\n",
    "#     csi_time_domain = csi_time_domain.reshape(83403, 32, 13)\n",
    "#     num_csi_instances = csi_time_domain.shape[0]\n",
    "\n",
    "#     y_real_all = np.zeros((num_csi_instances, B, W + C), dtype=complex)\n",
    "#     H_pert_all = np.zeros((num_csi_instances, B, W), dtype=complex)\n",
    "\n",
    "#     for i in range(num_csi_instances):\n",
    "#         H_real = csi_time_domain[i]\n",
    "#         for b in range(B):\n",
    "#             y_real = convolve(H_real[b], transmit_signal, mode='full')[:W + C]\n",
    "#             noise = np.sqrt(noise_power / 2) * (np.random.randn(W + C) + 1j * np.random.randn(W + C))\n",
    "#             y_real_noisy = y_real + noise\n",
    "#             y_real_no_cp = y_real_noisy[C:]\n",
    "\n",
    "#             y_freq = np.fft.fft(y_real_no_cp)\n",
    "#             s_freq = np.fft.fft(time_domain_signal)\n",
    "\n",
    "#             H_pert_all[i, b, :] = y_freq / s_freq\n",
    "#             H_pert_time = np.fft.ifft(H_pert_all[i, b, :])[:L]\n",
    "#             csi_time_domain[i, b, :] = H_pert_time\n",
    "\n",
    "#     csi_time_domain = csi_time_domain.reshape(83403, 4, 8, 13)\n",
    "#     reshaped_csi = csi_time_domain.reshape(83403, 32, 13)\n",
    "#     features_F1 = np.array([feature_extraction_F1(csi) for csi in reshaped_csi])\n",
    "\n",
    "#     x_noise = model.predict(features_F1)\n",
    "\n",
    "#     mae_noise = np.mean(np.abs(x_noise - groundtruth_maps))\n",
    "\n",
    "#     mae_noise_list.append(mae_noise)\n",
    "\n",
    "# # Plot MAE vs Perturbation Length\n",
    "# plt.plot(lp_values, mae_noise_list, label='MAE (Noisy Features)', marker='o')\n",
    "# plt.xlabel('Perturbation Length (Lp)')\n",
    "# plt.ylabel('Mean Absolute Error (MAE)')\n",
    "# plt.title('MAE vs Perturbation Length')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28be5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve\n",
    "\n",
    "mae_noise_list = []\n",
    "lp_values = []\n",
    "\n",
    "# Constants\n",
    "MAX_SAMPLES = 10000\n",
    "BATCH_SIZE = 1024\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Preprocess data\n",
    "groundtruth_maps, est_positions, grid_points = generate_all_probability_maps(groundtruth_positions)\n",
    "\n",
    "# Reduce data size\n",
    "csi_time_domain = csi_time_domain[:MAX_SAMPLES]\n",
    "groundtruth_maps = groundtruth_maps[:MAX_SAMPLES]\n",
    "\n",
    "for x in range(1, 200):\n",
    "    lp_values.append(x)\n",
    "\n",
    "    # Parameters\n",
    "    W = 1024\n",
    "    modulation_order = 16\n",
    "    C = W // 8\n",
    "    L = 13\n",
    "    Lp = x\n",
    "    noise_power = 0.1\n",
    "    B = 32\n",
    "\n",
    "    # QAM generation\n",
    "    data = np.random.randint(0, modulation_order, W)\n",
    "    qam_symbols = (2 * (data % 4) - 3) + 1j * (2 * (data // 4) - 3)\n",
    "    qam_symbols /= np.sqrt(10)\n",
    "    time_domain_signal = np.fft.ifft(qam_symbols)\n",
    "\n",
    "    # Perturbation generation\n",
    "    A_k = np.random.normal(0.0, 1.0, size=Lp)\n",
    "    phi_k = np.random.normal(0.0, 1.0, size=Lp)\n",
    "    p_bar = A_k * np.exp(1j * phi_k)\n",
    "    p_bar /= np.linalg.norm(p_bar)\n",
    "\n",
    "    perturbed_signal = np.convolve(p_bar, time_domain_signal, mode=\"full\")[:W]\n",
    "    cyclic_prefix = perturbed_signal[-C:]\n",
    "    transmit_signal = np.concatenate([cyclic_prefix, perturbed_signal])\n",
    "\n",
    "    # Reshape CSI\n",
    "    csi_time_domain = csi_time_domain.reshape(MAX_SAMPLES, 32, 13)\n",
    "\n",
    "    # Perturb CSI\n",
    "    for i in range(MAX_SAMPLES):\n",
    "        H_real = csi_time_domain[i]\n",
    "        for b in range(B):\n",
    "            y_real = convolve(H_real[b], transmit_signal, mode='full')[:W + C]\n",
    "            noise = np.sqrt(noise_power / 2) * (np.random.randn(W + C) + 1j * np.random.randn(W + C))\n",
    "            y_real_noisy = y_real + noise\n",
    "            y_real_no_cp = y_real_noisy[C:]\n",
    "\n",
    "            y_freq = np.fft.fft(y_real_no_cp)\n",
    "            s_freq = np.fft.fft(time_domain_signal)\n",
    "\n",
    "            H_freq = y_freq / s_freq\n",
    "            H_time = np.fft.ifft(H_freq)[:L]\n",
    "            csi_time_domain[i, b, :] = H_time\n",
    "\n",
    "    # Reshape for feature extraction\n",
    "    csi_time_domain = csi_time_domain.reshape(MAX_SAMPLES, 4, 8, 13)\n",
    "    reshaped_csi = csi_time_domain.reshape(MAX_SAMPLES, 32, 13)\n",
    "\n",
    "    # Feature extraction\n",
    "    features_F1 = np.array([feature_extraction_F1(csi) for csi in reshaped_csi])\n",
    "\n",
    "    # Convert to TensorFlow Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(features_F1)\n",
    "    dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "    # Predict in batches using GPU\n",
    "    x_noise = model.predict(dataset, verbose=0)\n",
    "\n",
    "    # Compute MAE\n",
    "    mae_noise = np.mean(np.abs(x_noise - groundtruth_maps))\n",
    "    mae_noise_list.append(mae_noise)\n",
    "\n",
    "# Plotting\n",
    "plt.plot(lp_values, mae_noise_list, label='MAE (Noisy Features)', marker='o')\n",
    "plt.xlabel('Perturbation Length (Lp)')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.title('MAE vs Perturbation Length')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e859da5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Summer_Intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
