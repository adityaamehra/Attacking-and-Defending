{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb75955",
   "metadata": {
    "id": "efb75955"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors, kneighbors_graph\n",
    "from sklearn.manifold import trustworthiness\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.manifold import MDS\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from tqdm.auto import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import queue\n",
    "import json\n",
    "\n",
    "\n",
    "# Antenna definitions\n",
    "ASSIGNMENTS = [\n",
    "\t[0, 13, 31, 29, 3, 7, 1, 12 ],\n",
    "\t[30, 26, 21, 25, 24, 8, 22, 15],\n",
    "\t[28, 5, 10, 14, 6, 2, 16, 18],\n",
    "\t[19, 4, 23, 17, 20, 11, 9, 27]\n",
    "]\n",
    "\n",
    "ANTENNACOUNT = np.sum([len(antennaArray) for antennaArray in ASSIGNMENTS])\n",
    "\n",
    "# Optimized version: Calibration constants precomputed once\n",
    "def load_calibrate_timedomain(path, offset_path):\n",
    "    with open(offset_path, \"r\") as offsetfile:\n",
    "        offsets = json.load(offsetfile)\n",
    "\n",
    "    # Precompute the calibration tensors\n",
    "    fft_len = 1024\n",
    "    freq_range = 2 * np.pi * np.arange(fft_len, dtype=np.float32) / fft_len\n",
    "    sto_offset = tf.constant(offsets[\"sto\"], dtype=tf.float32)[:, None] * freq_range\n",
    "    cpo_offset = tf.constant(offsets[\"cpo\"], dtype=tf.float32)[:, None]\n",
    "    calib_multiplier = tf.exp(tf.complex(0.0, sto_offset + cpo_offset))\n",
    "\n",
    "    def record_parse_function(proto):\n",
    "        record = tf.io.parse_single_example(proto, {\n",
    "            \"csi\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"pos-tachy\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"time\": tf.io.FixedLenFeature([], tf.float32),\n",
    "        })\n",
    "\n",
    "        csi = tf.io.parse_tensor(record[\"csi\"], out_type=tf.float32)\n",
    "        csi = tf.complex(csi[:, :, 0], csi[:, :, 1])\n",
    "        csi = tf.signal.fftshift(csi, axes=1)\n",
    "\n",
    "        position = tf.io.parse_tensor(record[\"pos-tachy\"], out_type=tf.float64)\n",
    "        time = record[\"time\"]\n",
    "        return csi, position[:2], time\n",
    "\n",
    "    def apply_calibration(csi, pos, time):\n",
    "        csi = tf.multiply(csi, calib_multiplier)\n",
    "        return csi, pos, time\n",
    "\n",
    "    def csi_time_domain(csi, pos, time):\n",
    "        csi = tf.signal.fftshift(tf.signal.ifft(tf.signal.fftshift(csi, axes=1)), axes=1)\n",
    "        return csi, pos, time\n",
    "\n",
    "    def cut_out_taps(tap_start, tap_stop):\n",
    "        def func(csi, pos, time):\n",
    "            return csi[:, tap_start:tap_stop], pos, time\n",
    "        return func\n",
    "\n",
    "    def order_by_antenna(csi, pos, time):\n",
    "        csi = tf.stack([tf.gather(csi, indices) for indices in ASSIGNMENTS])\n",
    "        return csi, pos, time\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(path)\n",
    "    dataset = dataset.map(record_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(apply_calibration, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(csi_time_domain, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(cut_out_taps(507, 520), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(order_by_antenna, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "inputpaths = [\n",
    "\t{\n",
    "\t\t\"tfrecords\" : \"dichasus/dichasus-cf02.tfrecords\",\n",
    "\t\t\"offsets\" : \"dichasus/reftx-offsets-dichasus-cf02.json\"\n",
    "\t},\n",
    "\t{\n",
    "\t\t\"tfrecords\" : \"dichasus/dichasus-cf03.tfrecords\",\n",
    "\t\t\"offsets\" : \"dichasus/reftx-offsets-dichasus-cf03.json\"\n",
    "\t},\n",
    "\t{\n",
    "\t\t\"tfrecords\" : \"dichasus/dichasus-cf04.tfrecords\",\n",
    "\t\t\"offsets\" : \"dichasus/reftx-offsets-dichasus-cf04.json\"\n",
    "\t}\n",
    "]\n",
    "\n",
    "full_dataset = load_calibrate_timedomain(inputpaths[0][\"tfrecords\"], inputpaths[0][\"offsets\"])\n",
    "\n",
    "for path in inputpaths[1:]:\n",
    "\tfull_dataset = full_dataset.concatenate(load_calibrate_timedomain(path[\"tfrecords\"], path[\"offsets\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d8746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Path to your HDF5 file\n",
    "file_path = '5G_dataset/5G_training_data.hdf5'\n",
    "\n",
    "# Create an empty dictionary to store NumPy arrays\n",
    "data_dict = {}\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File(file_path, 'r') as hdf:\n",
    "    # Iterate over all datasets in the file\n",
    "    for key in hdf.keys():\n",
    "        dataset = hdf[key][:]\n",
    "        data_dict[key] = dataset  # Store each dataset into the dictionary\n",
    "\n",
    "# Now, you can access individual datasets as NumPy arrays\n",
    "A_ID = data_dict['A_ID']\n",
    "B_ID = data_dict['B_ID']\n",
    "CIR_I = data_dict['CIR_I']\n",
    "CIR_R = data_dict['CIR_R']\n",
    "POS_X = data_dict['POS_X']\n",
    "POS_Y = data_dict['POS_Y']\n",
    "TD = data_dict['TD']\n",
    "TD_OFFSET = data_dict['TD_OFFSET']\n",
    "TIME_STAMP = data_dict['TIME_STAMP']\n",
    "\n",
    "# For verification, print shapes of arrays\n",
    "for key, value in data_dict.items():\n",
    "    print(f\"{key} shape: {value.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3689514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIR=CIR_R+ 1j*CIR_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0a296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e10c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a6428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_X,POS_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "grt_pos=np.stack((POS_X, POS_Y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ba2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grt_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf28e7",
   "metadata": {
    "id": "3aaf28e7"
   },
   "outputs": [],
   "source": [
    "groundtruth_positions = []\n",
    "csi_time_domain = []\n",
    "timestamps = []\n",
    "for csi, pos, time in full_dataset.batch(1000):\n",
    "\tcsi_time_domain.append(csi.numpy())\n",
    "\tgroundtruth_positions.append(pos.numpy())\n",
    "\ttimestamps.append(time.numpy())\n",
    "\n",
    "csi_time_domain = np.concatenate(csi_time_domain)\n",
    "groundtruth_positions = np.concatenate(groundtruth_positions)\n",
    "timestamps = np.concatenate(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "csi_time_domain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth_positions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c541917",
   "metadata": {},
   "outputs": [],
   "source": [
    "csi_time_domain[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f654351",
   "metadata": {
    "id": "3f654351"
   },
   "outputs": [],
   "source": [
    "def feature_extraction_F1(csi_matrix):\n",
    "    csi_ifft = np.fft.ifft(csi_matrix, axis=1)\n",
    "    R = csi_ifft * np.conj(csi_ifft)\n",
    "    r = R.flatten()\n",
    "    r_real = np.real(r)\n",
    "    r_imag = np.imag(r)\n",
    "    r_R = np.concatenate((r_real, r_imag))\n",
    "    feature_vector = r_R / np.linalg.norm(r_R)\n",
    "    return feature_vector\n",
    "m=csi_time_domain.shape[0]\n",
    "reshaped_csi = csi_time_domain.reshape(m, 32, 13)\n",
    "features_F1 = np.array([feature_extraction_F1(csi) for csi in reshaped_csi])\n",
    "print(\"Shape of Features (F1):\", features_F1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd37c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_csi[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ea0350",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIR[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c091e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_F1_cir(cir_matrix):\n",
    "    R = cir_matrix * np.conj(cir_matrix)\n",
    "    r = R.flatten()\n",
    "    r_real = np.real(r)\n",
    "    r_imag = np.imag(r)\n",
    "    r_R = np.concatenate((r_real, r_imag))\n",
    "    feature_vector = r_R / np.linalg.norm(r_R)\n",
    "    return feature_vector\n",
    "\n",
    "features_F1_cir = np.array([feature_extraction_F1_cir(cir) for cir in CIR])\n",
    "print(\"Shape of Features (F1_cir):\", features_F1_cir.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c368b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_F1_cir[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3ab63",
   "metadata": {
    "id": "8bc3ab63"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, x1, y_train, y1 = train_test_split(features_F1_cir, grt_pos, test_size=0.4, random_state=42)\n",
    "X_test, X_pred, y_test, y_pred = train_test_split(x1, y1, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98167351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ground truth positions\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(grt_pos[:, 0], grt_pos[:, 1], c='blue', label='Ground Truth', alpha=0.5)\n",
    "plt.title('Ground Truth Positions')\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "def build_grid(grid_size=30, space_bounds=None):\n",
    "    (xmin, xmax), (ymin, ymax) = space_bounds\n",
    "    x_lin = np.linspace(xmin, xmax, grid_size)\n",
    "    y_lin = np.linspace(ymin, ymax, grid_size)\n",
    "        \n",
    "    xv, yv = np.meshgrid(x_lin, y_lin)\n",
    "    grid_points = np.stack([xv.ravel(), yv.ravel()], axis=1)\n",
    "    return grid_points\n",
    "\n",
    "def compute_soft_probability_maps(positions, grid_points, temperature=.1):\n",
    "    N = positions.shape[0]\n",
    "    K = grid_points.shape[0]\n",
    "    dists = np.linalg.norm(positions[:, np.newaxis, :] - grid_points[np.newaxis, :, :], axis=2)\n",
    "    sim_scores = -dists / temperature\n",
    "    prob_maps = softmax(sim_scores, axis=1)\n",
    "    return prob_maps\n",
    "\n",
    "def estimate_positions_from_maps(prob_maps, grid_points):\n",
    "    return np.matmul(prob_maps, grid_points)\n",
    "\n",
    "def generate_all_probability_maps(positions, grid_size=30, temperature=.1):\n",
    "    x_min, x_max = positions[:, 0].min(), positions[:, 0].max()\n",
    "    y_min, y_max = positions[:, 1].min(), positions[:, 1].max()\n",
    "    print(\"Position bounds:\", (x_min, x_max), (y_min, y_max))\n",
    "    grid_points = build_grid(grid_size, space_bounds=((x_min, x_max), (y_min, y_max)))\n",
    "    print(\"Grid points shape:\", grid_points.shape)\n",
    "    prob_maps = compute_soft_probability_maps(positions, grid_points, temperature=temperature)\n",
    "\n",
    "    est_locations = estimate_positions_from_maps(prob_maps, grid_points)\n",
    "\n",
    "    return prob_maps, est_locations, grid_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e60806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(prob_map, grid_points, title=\"Heatmap\"):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(prob_map.reshape(30, 30), extent=(grid_points[:, 0].min(), grid_points[:, 0].max(),\n",
    "                                                  grid_points[:, 1].min(), grid_points[:, 1].max()),\n",
    "               origin='lower', cmap='hot', alpha=0.7)\n",
    "    plt.colorbar(label='Probability')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5bbe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_map_test, est_locations_test, grid_points_test = generate_all_probability_maps(y_test, grid_size=100, temperature=0.1)\n",
    "prob_map_train, est_locations_train, grid_points_train = generate_all_probability_maps(y_train, grid_size=100, temperature=0.1)\n",
    "prob_map_pred, est_locations_pred, grid_points_pred = generate_all_probability_maps(y_pred, grid_size=100, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(y_test[:, 0], y_test[:, 1], c='blue', label='Ground Truth (Test)', alpha=0.5)\n",
    "plt.scatter(est_locations_test[:, 0], est_locations_test[:, 1], c='red', label='Estimated (Test)', alpha=0.5)\n",
    "plt.title('Estimated vs Ground Truth Positions (Test Set)')\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cbb3a0",
   "metadata": {
    "id": "15cbb3a0"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "class NeuralNetwork(tf.keras.Model):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = layers.Dense(2048, input_dim=input_size)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.fc2 = layers.Dense(1024)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        # self.fc3 = layers.Dense(512)\n",
    "        # self.fc4 = layers.Dense(512)\n",
    "        # self.fc5 = layers.Dense(512)\n",
    "        self.fc3 = layers.Dense(output_size, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Set output_size to the number of grid points (e.g., 900 for 30x30 grid)\n",
    "output_size = grid_points_train.shape[0]  # grid_points from your earlier code\n",
    "print(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffb1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d48677",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_map_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec2ed3",
   "metadata": {
    "id": "7eec2ed3"
   },
   "outputs": [],
   "source": [
    "# predicted_maps = model.predict(X_pred)\n",
    "# plot_heatmap(predicted_maps[2], grid_points_pred, title=\"Predicted Heatmap for First Test Position\")\n",
    "# print(f\"Estimated :- {estimate_positions_from_maps(predicted_maps, grid_points_pred)[2]}\")\n",
    "# print(f\"Ground Truth :- {y_pred[2]}\")\n",
    "# plot_heatmap(prob_map_pred[2], grid_points_pred, title=\"Heatmap for First Test Position\")\n",
    "# predicted_positions = np.matmul(predicted_maps, grid_points_pred)\n",
    "# mae = np.mean(np.abs(y_pred - predicted_positions), axis=0)\n",
    "# print(\"Mean Absolute Error (Position Predictions):\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7d647",
   "metadata": {
    "id": "3cf7d647"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras.layers import Input, Dense, BatchNormalization, Dropout\n",
    "# from keras.models import Model\n",
    "\n",
    "# def build_denoising_autoencoder(input_dim):\n",
    "#     \"\"\"\n",
    "#     Builds a denoising autoencoder model.\n",
    "\n",
    "#     Args:\n",
    "#         input_dim: Dimension of the input features (e.g., 832 for features_F1).\n",
    "\n",
    "#     Returns:\n",
    "#         autoencoder: The denoising autoencoder model.\n",
    "#     \"\"\"\n",
    "#     # Input layer\n",
    "#     input_data = Input(shape=(input_dim,), name=\"input\")\n",
    "\n",
    "#     # Encoder\n",
    "#     x = Dense(512, activation=\"relu\", name=\"encoder_dense_1\")(input_data)\n",
    "#     x = BatchNormalization(name=\"encoder_bn_1\")(x)\n",
    "#     x = Dropout(0.2, name=\"encoder_dropout_1\")(x)\n",
    "\n",
    "#     x = Dense(256, activation=\"relu\", name=\"encoder_dense_2\")(x)\n",
    "#     x = BatchNormalization(name=\"encoder_bn_2\")(x)\n",
    "#     x = Dropout(0.2, name=\"encoder_dropout_2\")(x)\n",
    "\n",
    "#     x = Dense(128, activation=\"relu\", name=\"encoder_dense_3\")(x)\n",
    "#     x = BatchNormalization(name=\"encoder_bn_3\")(x)\n",
    "#     x = Dropout(0.2, name=\"encoder_dropout_3\")(x)\n",
    "\n",
    "#     x = Dense(64, activation=\"relu\", name=\"encoder_dense_4\")(x)\n",
    "#     x = BatchNormalization(name=\"encoder_bn_4\")(x)\n",
    "#     x = Dropout(0.2, name=\"encoder_dropout_4\")(x)\n",
    "\n",
    "#     encoded = Dense(32, activation=\"relu\", name=\"encoder_latent\")(x)  # Latent space\n",
    "\n",
    "#     # Decoder\n",
    "#     x = Dense(64, activation=\"relu\", name=\"decoder_dense_1\")(encoded)\n",
    "#     x = BatchNormalization(name=\"decoder_bn_1\")(x)\n",
    "#     x = Dropout(0.2, name=\"decoder_dropout_1\")(x)\n",
    "\n",
    "#     x = Dense(128, activation=\"relu\", name=\"decoder_dense_2\")(x)\n",
    "#     x = BatchNormalization(name=\"decoder_bn_2\")(x)\n",
    "#     x = Dropout(0.2, name=\"decoder_dropout_2\")(x)\n",
    "\n",
    "#     x = Dense(256, activation=\"relu\", name=\"decoder_dense_3\")(x)\n",
    "#     x = BatchNormalization(name=\"decoder_bn_3\")(x)\n",
    "#     x = Dropout(0.2, name=\"decoder_dropout_3\")(x)\n",
    "\n",
    "#     x = Dense(512, activation=\"relu\", name=\"decoder_dense_4\")(x)\n",
    "#     x = BatchNormalization(name=\"decoder_bn_4\")(x)\n",
    "#     x = Dropout(0.2, name=\"decoder_dropout_4\")(x)\n",
    "\n",
    "#     decoded = Dense(input_dim, activation=\"linear\", name=\"decoder_output\")(x)  # Reconstruct clean features\n",
    "\n",
    "#     # Autoencoder model\n",
    "#     autoencoder = Model(input_data, decoded, name=\"DenoisingAutoencoder\")\n",
    "\n",
    "#     return autoencoder\n",
    "\n",
    "# # Build the denoising autoencoder\n",
    "# input_dim = 832  # Dimension of the input features\n",
    "# autoencoder = build_denoising_autoencoder(input_dim)\n",
    "\n",
    "# # Compile the autoencoder\n",
    "# autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# # Print the autoencoder summary\n",
    "# autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f02dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_points = build_grid(grid_size=30, space_bounds=((groundtruth_positions[:, 0].min(), groundtruth_positions[:, 0].max()),\n",
    "                                                    (groundtruth_positions[:, 1].min(), groundtruth_positions[:, 1].max())))\n",
    "# Plot the grid points\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(grid_points[:, 0], grid_points[:, 1], c='red', label='Grid Points', alpha=0.5)\n",
    "plt.scatter(groundtruth_positions[:, 0], groundtruth_positions[:, 1], c='blue', label='Ground Truth', alpha=0.5)\n",
    "plt.title('Grid Points and Ground Truth Positions')\n",
    "plt.xlabel('X Position')\n",
    "plt.ylabel('Y Position')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f424b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "csi_time_domain_original = csi_time_domain\n",
    "groundtruth_positions_original = groundtruth_positions\n",
    "features_F1_original=features_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2339971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_map_train_original = prob_map_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d516897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve\n",
    "from keras.callbacks import EarlyStopping\n",
    "num_runs = 1  # or however many runs you want\n",
    "num_lp = 5     # since lp_values = [1, 2, 3, 4]\n",
    "\n",
    "mae_noise_runs = np.zeros((num_runs, num_lp))\n",
    "\n",
    "for run in range(num_runs):\n",
    "    mae_noise_list = []\n",
    "    mae_denoise_list = []\n",
    "    lp_values = []\n",
    "    temp=20000\n",
    "\n",
    "\n",
    "    csi_time_domain = csi_time_domain_original\n",
    "    groundtruth_positions = groundtruth_positions_original\n",
    "\n",
    "    prob_map_train = prob_map_train_original\n",
    "\n",
    "    prob_map_train = prob_map_train[:temp,:]\n",
    "\n",
    "    csi_time_domain=csi_time_domain[:temp,:,:]\n",
    "    groundtruth_positions= groundtruth_positions[:temp,:]\n",
    "\n",
    "    for x in range(1, 50,10):\n",
    "        lp_values.append(x)\n",
    "\n",
    "        # Parameters\n",
    "        W = 1024\n",
    "        modulation_order = 16\n",
    "        C = W // 8\n",
    "        L = 13\n",
    "        Lp = x\n",
    "        noise_power = 0.1\n",
    "        B = 32\n",
    "\n",
    "        # Generate frequency-domain symbols\n",
    "        data = np.random.randint(0, modulation_order, W)\n",
    "        qam_symbols = (2 * (data % 4) - 3) + 1j * (2 * (data // 4) - 3)\n",
    "        qam_symbols /= np.sqrt(10)\n",
    "\n",
    "        time_domain_signal = np.fft.ifft(qam_symbols)\n",
    "\n",
    "        # Zero-mean Gaussian for amplitude\n",
    "        A_k = np.random.normal(loc=0.0, scale=1.0, size=Lp)\n",
    "        # Zero-mean Gaussian for phase\n",
    "        phi_k = np.random.normal(loc=0.0, scale=1.0, size=Lp)\n",
    "\n",
    "        p_bar = A_k * np.exp(1j * phi_k)\n",
    "        # p_bar /= np.linalg.norm(p_bar)\n",
    "\n",
    "        perturbed_signal = np.convolve(p_bar, time_domain_signal, mode=\"full\")[:W]\n",
    "        cyclic_prefix = perturbed_signal[-C:]\n",
    "        transmit_signal = np.concatenate([cyclic_prefix, perturbed_signal])\n",
    "\n",
    "        csi_time_domain = csi_time_domain.reshape(temp, 32, 13)\n",
    "        num_csi_instances = csi_time_domain.shape[0]\n",
    "\n",
    "        y_real_all = np.zeros((num_csi_instances, B, W + C), dtype=complex)\n",
    "        H_pert_all = np.zeros((num_csi_instances, B, W), dtype=complex)\n",
    "\n",
    "        for i in range(num_csi_instances):\n",
    "            H_real = csi_time_domain[i]\n",
    "            for b in range(B):\n",
    "                y_real = convolve(H_real[b], transmit_signal, mode='full')[:W + C]\n",
    "                noise = np.sqrt(noise_power / 2) * (np.random.randn(W + C) + 1j * np.random.randn(W + C))\n",
    "                y_real_noisy = y_real + noise\n",
    "                y_real_no_cp = y_real_noisy[C:]\n",
    "\n",
    "                y_freq = np.fft.fft(y_real_no_cp)\n",
    "                s_freq = np.fft.fft(time_domain_signal)\n",
    "\n",
    "                H_pert_all[i, b, :] = y_freq / s_freq\n",
    "                H_pert_time = np.fft.ifft(H_pert_all[i, b, :])[:L]\n",
    "                csi_time_domain[i, b, :] = H_pert_time\n",
    "\n",
    "        csi_time_domain = csi_time_domain.reshape(temp, 4, 8, 13)\n",
    "        reshaped_csi = csi_time_domain.reshape(temp, 32, 13)\n",
    "        features_F1 = np.array([feature_extraction_F1(csi) for csi in reshaped_csi])\n",
    "        X_train = features_F1\n",
    "        y_train = features_F1_original[:temp,:]\n",
    "\n",
    "        # history = autoencoder.fit(\n",
    "        #     X_train, y_train,\n",
    "        #     epochs=30,\n",
    "        #     batch_size=32,\n",
    "        #     validation_split=0.2,\n",
    "        #     verbose=0\n",
    "        # )\n",
    "        print(features_F1.shape)\n",
    "        model = NeuralNetwork(input_size=832, output_size=output_size)\n",
    "\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "        \n",
    "        # early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=0, restore_best_weights=True)\n",
    "        print(X_train.shape, prob_map_train.shape)\n",
    "        history = model.fit(X_train, prob_map_train, epochs=50, batch_size=128, validation_split=0.2)\n",
    "\n",
    "        x_noise = model.predict(features_F1)\n",
    "        # features_denoise = autoencoder.predict(features_F1)\n",
    "        # x_denoise = model.predict(features_denoise)\n",
    "        # x_denoise = np.matmul(x_denoise, grid_points)\n",
    "        x_noise = np.matmul(x_noise, grid_points)\n",
    "        mae_noise = np.mean(np.sqrt((groundtruth_positions[:, 0] - x_noise[:, 0])**2 + (groundtruth_positions[:, 1] - x_noise[:, 1])**2))\n",
    "        # mae_denosie = np.mean(np.sqrt((groundtruth_positions[:, 0] - x_denoise[:, 0])**2 + (groundtruth_positions[:, 1] - x_denoise[:, 1])**2))\n",
    "\n",
    "        mae_noise_list.append(mae_noise)\n",
    "        # mae_denoise_list.append(mae_denosie)\n",
    "\n",
    "# Plot MAE vs Perturbation Length\n",
    "mae_noise_runs= np.mean(mae_noise_runs, axis=0)\n",
    "plt.plot(lp_values, mae_noise_list, label='MAE (Noisy Features)', marker='o')\n",
    "# plt.plot(lp_values, mae_denoise_list, label='MAE (Denoised Features)', marker='s')\n",
    "plt.xlabel('Perturbation Length (Lp)')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.title('MAE vs Perturbation Length')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb591e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Summer_Intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
